{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax.scipy.stats import multivariate_normal\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define our sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Target and Hamiltonian.** We wish to sample from a posterior $\\pi(q)$ on position $q \\in \\mathbb{R}^d$. HMC augments state with momentum $p \\in \\mathbb{R}^d$ and samples from the joint density\n",
        "$$\n",
        "\\pi(p, q) \\propto \\exp(-H(p,q)), \\qquad H(p,q) = K(p) + U(q).\n",
        "$$\n",
        "\n",
        "**Kinetic and potential energy:**\n",
        "$$\n",
        "K(p) = \\frac{1}{2} p^\\top M^{-1} p,\n",
        "$$\n",
        "$$\n",
        "U(q) = -\\log \\pi(q).\n",
        "$$\n",
        "\n",
        "**Notation.** \n",
        "\n",
        "We write $\\mathcal{N}(x; \\mu, \\Sigma)$ for the Gaussian with mean\n",
        "$\\mu$ and covariance matrix $\\Sigma$. \n",
        "\n",
        "Thus $\\pi(p \\mid q) = \\mathcal{N}(p; 0, M)$ is the conditional distribution of momentum given position: mean $0$, covariance $M$. Here $M$ is the mass matrix (positive definite). Then $\\pi(q)$ is unchanged by marginalization.\n",
        "\n",
        "**Transition.** One HMC step: (1) draw $p \\sim \\mathcal{N}(0, M)$; (2) integrate Hamilton's equations for $(q,p)$ with a reversible, volume-preserving integrator (leapfrog) for $L$ steps of size $\\Delta t$; (3) optionally apply a Metropolis correction on $H$ (omitted in this demo). The code below implements $H$, $K$, $U$, and the leapfrog integrator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for visual purposes, no metropolis correction! see the metropolis notebook.\n",
        "class hmc():\n",
        "    def __init__(self, dim, logpi_q, mass, PRNGkey, dt=0.1, L=10):\n",
        "        # Basic attr.\n",
        "        self.dim = dim\n",
        "        self.dt = dt\n",
        "        self.L = L\n",
        "        self.PRNGkey = PRNGkey\n",
        "        \n",
        "        # Read the position space posterior\n",
        "        # (Potential energy)\n",
        "        self.logpi_q = logpi_q\n",
        "        self.negdV = grad(self.logpi_q)\n",
        "        \n",
        "        # Construct the momentum sampler\n",
        "        self.mass = mass\n",
        "        self.massinv = jnp.linalg.inv(mass)\n",
        "        \n",
        "    def logpi_p_q(self, p, q):\n",
        "        # the log pi(p|q) (Kinetic energy)\n",
        "        # -dim/2*log(2 pi) - (p^T M^-1 p) / 2\n",
        "        return multivariate_normal.logpdf(p, \n",
        "                                            mean=jnp.zeros(self.dim), \n",
        "                                            cov=self.massinv)\n",
        "    \n",
        "    # Construct the Hamiltonian\n",
        "    def H(self, p, q):\n",
        "        return -self.logpi_p_q(p,q)-self.logpi_q(q)\n",
        "    \n",
        "    # ----------------\n",
        "    # Define the leapfrog integrator\n",
        "    @partial(jit, static_argnums=(0,)) # partial jit for class type argument at 0\n",
        "    def _leapfrog_incre(self, q0, p0):\n",
        "        # update momentum by half time step\n",
        "        p_halfdt = p0 + self.dt/2*self.negdV(q0)\n",
        "        # update position by full time step\n",
        "        q_dt = q0 + self.dt * self.massinv@p_halfdt\n",
        "        # update momentum by full time step\n",
        "        p_dt = p_halfdt + self.dt/2*self.negdV(q_dt)\n",
        "        return q_dt,p_dt\n",
        "    \n",
        "    def _leapfrog(self, q0, p0):\n",
        "        # integrate L times\n",
        "        for _ in range(self.L):\n",
        "            q0,p0 = self._leapfrog_incre(q0,p0)\n",
        "        return q0,p0\n",
        "    \n",
        "    # Generate 1 sample\n",
        "    def next_sample(self, q0):\n",
        "        self.PRNGkey, subkey = random.split(self.PRNGkey)\n",
        "        p0 = random.multivariate_normal(key=subkey, mean=jnp.zeros(self.dim), cov=self.mass, method='cholesky')\n",
        "        qf, pf = self._leapfrog(q0,p0)\n",
        "        return qf\n",
        "\n",
        "    # Generate N samples\n",
        "    def next_Nsample(self, q0, N):\n",
        "        qs = -999*jnp.ones((N,self.dim))\n",
        "        for i in range(N):\n",
        "            qs=qs.at[i].set(q0)\n",
        "            q0 = self.next_sample(q0)\n",
        "        return qs\n",
        "    \n",
        "    # ----------------\n",
        "    # The verbose integrator\n",
        "    def _leapfrog_verbose(self, q0, p0):\n",
        "        # integrate L times\n",
        "        qs = -999*jnp.ones((self.L,self.dim))\n",
        "        ps = -999*jnp.ones((self.L,self.dim))\n",
        "        for i in range(self.L):\n",
        "            qs=qs.at[i].set(q0)\n",
        "            ps=ps.at[i].set(p0)\n",
        "            q0,p0 = self._leapfrog_incre(q0,p0)\n",
        "        # return not only the final state but also the history\n",
        "        return q0,p0,qs,ps\n",
        "    \n",
        "    # Generate 1 sample\n",
        "    def next_sample_verbose(self, q0):\n",
        "        self.PRNGkey, subkey = random.split(self.PRNGkey)\n",
        "        p0 = random.multivariate_normal(key=subkey, mean=jnp.zeros(self.dim), cov=self.mass, method='cholesky')\n",
        "        qf, pf, qs, ps = self._leapfrog_verbose(q0,p0)\n",
        "        return qf, pf, qs, ps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define our posterior function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Test posterior (Gaussian).** We use a 2D normal so that the sampler and trajectories are easy to see:\n",
        "$$\n",
        "\\log \\pi(q) = -\\frac{d}{2}\\log(2\\pi) - \\frac{1}{2}\\log|\\Sigma| - \\frac{1}{2}(q - \\mu)^\\top \\Sigma^{-1}(q - \\mu).\n",
        "$$\n",
        "Here $\\mu = (0,0)^\\top$ and $\\Sigma = \\operatorname{diag}(0.8, 1)$. \n",
        "\n",
        "The function `logpi_q(q)` returns this $\\log \\pi(q)$; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def logpi_q(q):\n",
        "    # the log posterior\n",
        "    return multivariate_normal.logpdf(q, jnp.array([0,0]), jnp.array([[0.8,0],[0,1]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: Log-posterior over (q1,q2) plane; elongated contours match Sigma = diag(0.8,1).\n",
        "x = jnp.linspace(-3,3,100)\n",
        "y = jnp.linspace(-3,3,100)\n",
        "x_grid, y_grid = jnp.meshgrid(x, y)\n",
        "z = logpi_q(\n",
        "    jnp.swapaxes(\n",
        "        jnp.swapaxes(jnp.array([x_grid,y_grid]),0,-1),\n",
        "        0,1)\n",
        "    )\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Leapfrog integrator.** Hamilton's equations:\n",
        "$$\n",
        "\\dot{q} = M^{-1} p,\n",
        "$$\n",
        "$$\n",
        "\\dot{p} = -\\nabla U(q).\n",
        "$$\n",
        "One step from $(q_0, p_0)$ with step size $\\Delta t$:\n",
        "$$\n",
        "p_{1/2} = p_0 + \\frac{\\Delta t}{2}\\, (-\\nabla U(q_0)),\n",
        "$$\n",
        "$$\n",
        "q_1 = q_0 + \\Delta t\\, M^{-1} p_{1/2},\n",
        "$$\n",
        "$$\n",
        "p_1 = p_{1/2} + \\frac{\\Delta t}{2}\\, (-\\nabla U(q_1)).\n",
        "$$\n",
        "We apply this $L$ times per HMC step; total simulated time is $L \\Delta t$. \n",
        "\n",
        "In\n",
        "the code, `negdV(q)` is $-\\nabla U(q)$; `_leapfrog_incre` performs one such\n",
        "triple update; `_leapfrog` chains $L$ steps. (Note: \n",
        "`negdV` in the sampler is $-\\nabla_q \\log \\pi(q) = \\Sigma^{-1}(q - \\mu)$.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leapfrog integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plotting helper func.\n",
        "def quiver_2dline(q,color='k'):\n",
        "    # q = [[x1, y1],[x2, y2],...]\n",
        "    x,y = q.T\n",
        "    plt.quiver(x[:-1], y[:-1], x[1:]-x[:-1], y[1:]-y[:-1], scale_units='xy', angles='xy', scale=1, color=color)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: One HMC trajectory from q0=(0,0); contour with leapfrog path. No Metropolis so endpoint always accepted.\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=jnp.eye(2),\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              L = 50)\n",
        "# generate 1 point starting from q0 = [0,0]\n",
        "qf, pf, qs, ps = sampler.next_sample_verbose(q0=jnp.zeros(2))\n",
        "print('first sample:', qf)\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "quiver_2dline(qs)\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tracing the trajectories (no Metropolis correction)\n",
        "- Notice the momentum resampling at each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**One-step transition (no Metropolis).** Each step: draw $p_0 \\sim \\mathcal{N}(0, M)$, then $(q', p') = \\text{Leapfrog}^L(q, p_0)$. The next state is $q'$; momentum is discarded and resampled at the next step. Without a Metropolis accept/reject, $H$ is not preserved and the marginal distribution of $q$ is only approximate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Mass matrix $M$.** The kinetic energy is $K(p) = \\frac{1}{2} p^\\top M^{-1} p$. In the leapfrog update, $q$ is updated by $\\Delta t\\, M^{-1} p_{1/2}$, so a larger $M$ (larger inertia) yields smaller position updates per step. Scaling $M$ changes the trajectory shape and effective step size in $q$; the code uses `mass` for $M$ and `massinv` for $M^{-1}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Integration length $L$.** The trajectory length in simulated time is $L \\Delta t$. Larger $L$ gives longer paths and potentially larger moves in $q$ per HMC step; too large $L$ can loop back and reduce acceptance (when Metropolis is used)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: N consecutive HMC steps; each path from previous endpoint with new random momentum. Chain drifts over posterior.\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=jnp.eye(2),\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 0.2,\n",
        "              L = 8)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(4):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: H along leapfrog path for five trajectories; wiggles show non-conservation (no Metropolis).\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    h = sampler.H(qs,ps)\n",
        "    plt.plot(h,color=f'C{i}')\n",
        "plt.xlabel('LF integration step')\n",
        "plt.ylabel('H')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below (top): Mass M=100I, five trajectories; heavier mass gives shorter, stiffer paths. (bottom): H along path.\n",
        "mass = jnp.eye(2)*100\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=mass,\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 0.2,\n",
        "              L = 30)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    h = sampler.H(qs,ps)\n",
        "    plt.plot(h,color=f'C{i}')\n",
        "plt.xlabel('LF integration step')\n",
        "plt.ylabel('H')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Integration error and Metropolis correction\n",
        "- Two cases, same $L \\Delta t$ = constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below (top): Large dt, L*dt=60 fixed; fewer, coarser steps, jagged paths. (bottom): H along path; strong energy drift.\n",
        "mass = jnp.eye(2)\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=mass,\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 1,\n",
        "              L = 60)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "qf = jnp.array([1.0,0.0])\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    h = sampler.H(qs,ps)\n",
        "    plt.plot(h,color=f'C{i}')\n",
        "plt.xlabel('LF integration step')\n",
        "plt.ylabel('H')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below (top): Small dt, same L*dt=60; many small steps, smoother paths. (bottom): H and -U; H flatter.\n",
        "mass = jnp.eye(2)\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=mass,\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 0.1,\n",
        "              L = 600)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "qf = jnp.array([1.0,0.0])\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(5):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    h = sampler.H(qs,ps)\n",
        "    v = -sampler.logpi_q(qs)\n",
        "    # plt.plot(v,color=f'C{i}',ls=\":\") # potential energy\n",
        "    plt.plot(h,color=f'C{i}')\n",
        "plt.xlabel('LF integration step')\n",
        "plt.ylabel('H')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NumPyro example\n",
        "\n",
        "**Same target.** \n",
        "\n",
        "We sample the same target\n",
        "\n",
        "- $\\pi(q) = \\mathcal{N}(q; \\mu, \\Sigma)$ with \n",
        "\n",
        "- $\\mu = (0,0)^\\top$ and \n",
        "\n",
        "- $\\Sigma = \\operatorname{diag}(0.8, 1)$. \n",
        "\n",
        "The model is defined by a single `numpyro.sample(\"q\", ...)`; with no observed\n",
        "data this is the posterior. \n",
        "\n",
        "NumPyro builds $\\log \\pi(q)$ and $\\nabla_q \\log \\pi(q)$ from the distribution, then runs HMC (or NUTS) with leapfrog and Metropolis.\n",
        "\n",
        "**Parameterization: Cholesky vs covariance.** \n",
        "\n",
        "We pass the Cholesky factor $L$ (lower triangular) with $\\Sigma = L L^\\top$\n",
        "rather than $\\Sigma$ itself. \n",
        "\n",
        "A mean-zero vector is drawn by \n",
        "- $z \\sim \\mathcal{N}(0, I)$\n",
        "-  $q = L z$\n",
        "-  The\n",
        "log-density uses $\\log \\det \\Sigma = 2 \\sum \\log(\\operatorname{diag}(L))$\n",
        "- the quadratic form $q^\\top \\Sigma^{-1} q = \\| L^{-1} q \\|^2$, which is computed\n",
        "by solving $L u = q$ (forward solve) for $u$ and then $\\|u\\|^2$.\n",
        "\n",
        "In high dimensions this is preferable: \n",
        "\n",
        "- (1) $L$ has only $d(d+1)/2$ free entries and guarantees $\\Sigma$ is positive\n",
        "definite; \n",
        "\n",
        "- (2) no explicit $d \\times d$ matrix inversion; \n",
        "\n",
        "- (3) forward/backward solves with triangular $L$ are numerically stable and\n",
        "$O(d^2)$ instead of $O(d^3)$ for matrix inversion; \n",
        "\n",
        "- (4) gradients for HMC stay in the $L$-space and avoid building full $\\Sigma$. Using `scale_tril=L` in NumPyro follows this parameterization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpyro\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import HMC, MCMC\n",
        "\n",
        "# Same target: q ~ N(mu, Sigma), mu=(0,0), Sigma = diag(0.8, 1); scale_tril = Cholesky L with Sigma = L L^T\n",
        "def model():\n",
        "    q = numpyro.sample(\"q\", dist.MultivariateNormal(\n",
        "        jnp.zeros(2),\n",
        "        scale_tril=jnp.linalg.cholesky(jnp.array([[0.8, 0.0], [0.0, 1.0]]))\n",
        "    ))\n",
        "    return q\n",
        "\n",
        "# HMC kernel: step size and number of steps (NumPyro uses NUTS by default in practice)\n",
        "kernel = HMC(model, step_size=0.2)\n",
        "mcmc = MCMC(kernel, num_warmup=500, num_samples=1000)\n",
        "rng = random.PRNGKey(0)\n",
        "mcmc.run(rng)\n",
        "\n",
        "samples = mcmc.get_samples()\n",
        "q_numpyro = samples[\"q\"]  # shape: (1000, 2)\n",
        "mcmc.print_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overlay NumPyro samples on the same Gaussian posterior contour\n",
        "_mean = jnp.zeros(2)\n",
        "_cov = jnp.array([[0.8, 0.0], [0.0, 1.0]])\n",
        "_x = jnp.linspace(-3, 3, 100)\n",
        "_y = jnp.linspace(-3, 3, 100)\n",
        "_xg, _yg = jnp.meshgrid(_x, _y)\n",
        "_points = jnp.stack([jnp.ravel(_xg), jnp.ravel(_yg)], axis=1)\n",
        "_z = vmap(lambda p: multivariate_normal.logpdf(p, _mean, _cov))(_points).reshape(_xg.shape)\n",
        "plt.contourf(_xg, _yg, _z)\n",
        "plt.scatter(q_numpyro[:, 0], q_numpyro[:, 1], alpha=0.3, s=5, c=\"C1\")\n",
        "plt.axis(\"scaled\")\n",
        "plt.xlabel(\"q1\")\n",
        "plt.ylabel(\"q2\")\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mixed Gaussian\n",
        "\n",
        "**Mixture posterior.** We switch to a three-component Gaussian mixture:\n",
        "$$\n",
        "\\pi(q) = \\sum_{k=1}^{3} w_k\\, \\mathcal{N}(q; \\mu_k, \\Sigma_k), \\qquad w_k = 1/3,\\ \\Sigma_k = 0.2\\, I.\n",
        "$$\n",
        "Means $\\mu_k$ are $(0,1)$, $(2,0)$, $(-1,-2)$. The redefined `logpi_q(q)`\n",
        "returns $\\log \\pi(q) = \\log \\sum_k w_k \\mathcal{N}(q;\\mu_k,\\Sigma_k)$; gradients\n",
        "are used by the leapfrog integrator as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: Posterior density for three-component mixture; three modes near (0,1), (2,0), (-1,-2).\n",
        "def logpi_q(q):\n",
        "    # the log posterior\n",
        "    res = multivariate_normal.pdf(q, jnp.array([0,1]), jnp.array([[0.2,0],[0,0.2]]))\n",
        "    res += multivariate_normal.pdf(q, jnp.array([2,0]), jnp.array([[0.2,0],[0,0.2]]))\n",
        "    res += multivariate_normal.pdf(q, jnp.array([-1,-2]), jnp.array([[0.2,0],[0,0.2]]))\n",
        "    return jnp.log(res)\n",
        "\n",
        "x = jnp.linspace(-5,5,100)\n",
        "y = jnp.linspace(-5,5,100)\n",
        "x_grid, y_grid = jnp.meshgrid(x, y)\n",
        "z = jnp.exp(logpi_q(\n",
        "    jnp.swapaxes(\n",
        "        jnp.swapaxes(jnp.array([x_grid,y_grid]),0,-1),\n",
        "        0,1)\n",
        "    ))\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: Fifteen HMC steps on mixture; whether chain reaches all three modes.\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=jnp.eye(2),\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 0.2,\n",
        "              L = 15)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(15):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Figure below: Twenty steps with default mass; compare coverage and mode-hopping.\n",
        "mass = jnp.array([[1,0],[0,1]])\n",
        "sampler = hmc(dim=2,\n",
        "              logpi_q=logpi_q,\n",
        "              mass=mass,\n",
        "              PRNGkey=random.PRNGKey(1),\n",
        "              dt = 0.2,\n",
        "              L = 30)\n",
        "\n",
        "h = plt.contourf(x_grid, y_grid, z)\n",
        "\n",
        "qf = jnp.zeros(2)\n",
        "for i in range(20):\n",
        "    # generate 1 point starting from q0 = [0,0]\n",
        "    qf, pf, qs, ps = sampler.next_sample_verbose(q0=qf)\n",
        "    print(f'q_{i}:', qf)\n",
        "    quiver_2dline(qs,color=f'C{i}')\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "jax-cosmo",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
